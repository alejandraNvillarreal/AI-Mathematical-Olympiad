{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":73231,"databundleVersionId":8365361,"sourceType":"competition"},{"sourceId":7369493,"sourceType":"datasetVersion","datasetId":4281572},{"sourceId":8012825,"sourceType":"datasetVersion","datasetId":4720595},{"sourceId":8023365,"sourceType":"datasetVersion","datasetId":4728129},{"sourceId":8052555,"sourceType":"datasetVersion","datasetId":4748944},{"sourceId":11261,"sourceType":"modelInstanceVersion","modelInstanceId":8332,"modelId":3301},{"sourceId":11264,"sourceType":"modelInstanceVersion","modelInstanceId":8318,"modelId":3301}],"dockerImageVersionId":30699,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":724.728315,"end_time":"2024-02-29T09:37:08.760349","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-02-29T09:25:04.032034","version":"2.5.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"21267b653022419eb6fc3f47aa4db8ed":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_926e7ccdad6440be85c76931860b744c","placeholder":"​","style":"IPY_MODEL_feef8334edb24f6da22e8bb1d8d80c67","value":"Loading checkpoint shards: 100%"}},"2144e851698b4707ad1c7fc29fe21b03":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3963993becfa487c9ff725f211915e67":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7a725e1b0cc4ad78a62beab5f663065","placeholder":"​","style":"IPY_MODEL_fdb32baaed7145d8a8024b615ef242ca","value":" 19/19 [10:48&lt;00:00, 33.24s/it]"}},"5882b6e860be4a0db012a64fc0704a3f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_21267b653022419eb6fc3f47aa4db8ed","IPY_MODEL_d91eb83d016a4381828192a98f798f9b","IPY_MODEL_3963993becfa487c9ff725f211915e67"],"layout":"IPY_MODEL_6a892a5561f742bb9db9f13859c18e90"}},"6a892a5561f742bb9db9f13859c18e90":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"926e7ccdad6440be85c76931860b744c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d91eb83d016a4381828192a98f798f9b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2144e851698b4707ad1c7fc29fe21b03","max":19,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e0693b32889c42b18b9a3844e045d048","value":19}},"e0693b32889c42b18b9a3844e045d048":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f7a725e1b0cc4ad78a62beab5f663065":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fdb32baaed7145d8a8024b615ef242ca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"feef8334edb24f6da22e8bb1d8d80c67":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}},"version_major":2,"version_minor":0}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Forked From https://www.kaggle.com/code/mbmmurad/submission-with-the-best-nb-new-api\n\ncredits:\n* https://www.kaggle.com/code/abdurrafae/improved-code-interpretation\n* https://www.kaggle.com/code/yuanwangzhang/updated-code-interpretation-n-repetitions-17/comments\n* https://www.kaggle.com/code/olyatsimboy/aimo-openmath-mistral-baseline\n* https://www.kaggle.com/code/aatiffraz/prompt-prediction-w-mixtral-mistral7b-gemma-llama\n* https://www.kaggle.com/code/thedrcat/aimo-mixtral-baseline","metadata":{"execution":{"iopub.status.busy":"2024-04-26T23:17:13.26597Z","iopub.execute_input":"2024-04-26T23:17:13.266288Z","iopub.status.idle":"2024-04-26T23:17:13.271529Z","shell.execute_reply.started":"2024-04-26T23:17:13.26626Z","shell.execute_reply":"2024-04-26T23:17:13.27058Z"}}},{"cell_type":"markdown","source":"This notebook is forked from [the Notebook](https://www.kaggle.com/code/abdurrafae/improved-code-interpretation) that won the early sharing prize by @[abdurrafae](https://www.kaggle.com/abdurrafae)\n\nI tried to use this notebook to submit with the new API","metadata":{}},{"cell_type":"code","source":"import time\n\nNOTEBOOK_START_TIME = time.time()","metadata":{"execution":{"iopub.status.busy":"2024-06-28T04:18:42.074559Z","iopub.execute_input":"2024-06-28T04:18:42.075316Z","iopub.status.idle":"2024-06-28T04:18:42.091886Z","shell.execute_reply.started":"2024-06-28T04:18:42.075289Z","shell.execute_reply":"2024-06-28T04:18:42.090933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Zero-shot MMOS-DeepSeekMath-7B with self-consistency and generated code reasoning evaluation\n\nSelf-consistency is a modification of the standard greedy decoding in reasoning pipelines via sampling several diverse answers followed by aggregation, e.g., most common answer ([SC-CoT paper](https://arxiv.org/pdf/2203.11171.pdf)).\n\nIn this kernel, we will consider MMOS-DeepSeekMath-7B RL-tuned backbone; in my experiments, this model produces more consistent code reasoning and the code block execution will allow us to decrease arithmetic hallucinations.","metadata":{}},{"cell_type":"code","source":"DEBUG = False\n\nQUANT = False\n\nif QUANT:\n    from transformers import BitsAndBytesConfig\n    quantization_config = BitsAndBytesConfig(\n        load_in_4bit = True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.bfloat16,\n        bnb_4bit_use_double_quant=True,\n    )\n\nUSE_PAST_KEY = True","metadata":{"papermill":{"duration":18.075198,"end_time":"2024-02-29T09:25:25.295954","exception":false,"start_time":"2024-02-29T09:25:07.220756","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-28T04:18:43.876269Z","iopub.execute_input":"2024-06-28T04:18:43.877013Z","iopub.status.idle":"2024-06-28T04:18:43.882764Z","shell.execute_reply.started":"2024-06-28T04:18:43.876973Z","shell.execute_reply":"2024-06-28T04:18:43.881599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nif QUANT:\n    !pip install -U /kaggle/input/accelerate-wheelwhl/accelerate-0.29.1-py3-none-any.whl -qq\n    !pip install -U /kaggle/input/bitsandbytes-0-42-0-py3-none-any-whl/bitsandbytes-0.42.0-py3-none-any.whl -qq\n\n\nimport torch\nimport gc\ntorch.backends.cuda.enable_mem_efficient_sdp(False)\n\nfrom transformers import (\n    AutoModelForCausalLM, \n    AutoTokenizer, \n    AutoConfig,\n    StoppingCriteria,\n    set_seed\n)\n\nimport transformers\nprint(f\"Transformers Version: {transformers.__version__}\")\nset_seed(42)","metadata":{"papermill":{"duration":18.075198,"end_time":"2024-02-29T09:25:25.295954","exception":false,"start_time":"2024-02-29T09:25:07.220756","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-28T04:18:46.115063Z","iopub.execute_input":"2024-06-28T04:18:46.115488Z","iopub.status.idle":"2024-06-28T04:19:01.459904Z","shell.execute_reply.started":"2024-06-28T04:18:46.115455Z","shell.execute_reply":"2024-06-28T04:19:01.458955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom tqdm import tqdm\nPRIVATE = True","metadata":{"papermill":{"duration":1.224774,"end_time":"2024-02-29T09:36:31.21757","exception":false,"start_time":"2024-02-29T09:36:29.992796","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-28T04:19:01.461852Z","iopub.execute_input":"2024-06-28T04:19:01.462762Z","iopub.status.idle":"2024-06-28T04:19:01.466847Z","shell.execute_reply.started":"2024-06-28T04:19:01.462727Z","shell.execute_reply":"2024-06-28T04:19:01.466040Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def naive_parse(answer):\n    out = []\n    start = False\n    end = False\n    for l in reversed(list(answer)):\n        if l in '0123456789' and not end:\n            start = True\n            out.append(l)\n        else:\n            if start:\n                end = True\n        \n    out = reversed(out)\n    return ''.join(out)","metadata":{"execution":{"iopub.status.busy":"2024-06-28T04:19:03.257466Z","iopub.execute_input":"2024-06-28T04:19:03.258227Z","iopub.status.idle":"2024-06-28T04:19:03.263715Z","shell.execute_reply.started":"2024-06-28T04:19:03.258199Z","shell.execute_reply":"2024-06-28T04:19:03.262678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport sys\nimport subprocess\n\ndef return_last_print(output, n):\n    lines = output.strip().split('\\n')\n    if lines:\n        return lines[n]\n    else:\n        return \"\"\n\ndef process_code(code, return_shell_output=False):\n    \n    def repl(match):\n        if \"real\" not in match.group():\n            return \"{}{}\".format(match.group()[:-1], ', real=True)')\n        else:\n            return \"{}{}\".format(match.group()[:-1], ')')\n    code = re.sub(r\"symbols\\([^)]+\\)\", repl, code)\n\n    if return_shell_output:\n        code = code.replace('\\n', '\\n    ')\n            # Add a try...except block\n        code = \"\\ntry:\\n    from sympy import *\\n{}\\nexcept Exception as e:\\n    print(e)\\n    print('FAIL')\\n\".format(code)\n    \n    if not return_shell_output:\n        print(code)\n    with open('code.py', 'w') as fout:\n        fout.write(code)\n    \n    batcmd = 'timeout 7 ' + sys.executable + ' code.py'\n    try:\n        shell_output = subprocess.check_output(batcmd, shell=True).decode('utf8')\n        return_value = return_last_print(shell_output, -1)\n        print(shell_output)\n        if return_shell_output:\n            if return_value=='FAIL':\n                CODE_STATUS = False\n                return_value = return_last_print(shell_output, -2)\n                if \"not defined\" in return_value:\n                    return_value+='\\nTry checking the formatting and imports'\n            else:\n                CODE_STATUS = True\n            return return_value, CODE_STATUS  \n        code_output = round(float(eval(return_value))) % 1000\n    except Exception as e:\n        print(e,'shell_output')\n        code_output = -1\n    \n    if return_shell_output:\n        if code_output==-1:\n            CODE_STATUS = False\n        else:\n            CODE_STATUS = True\n        return code_output, CODE_STATUS  \n    \n    \n    return code_output\n\n\ndef process_text_output(output):\n    result = output    \n    try:\n        result_output = re.findall(r'\\\\boxed\\{(\\d+)\\}', result)\n\n        print('BOXED', result_output)\n        if not len(result_output):\n            result_output = naive_parse(result)\n        else:\n            result_output = result_output[-1]\n\n        print('BOXED FINAL', result_output)\n        if not len(result_output):\n            result_output = -1\n        \n        else:\n            result_output = round(float(eval(result_output))) % 1000\n    \n    except Exception as e:\n        print(e)\n        print('ERROR PARSING TEXT')\n        result_output = -1\n    \n    return result_output\n","metadata":{"execution":{"iopub.status.busy":"2024-06-28T04:19:04.956068Z","iopub.execute_input":"2024-06-28T04:19:04.956763Z","iopub.status.idle":"2024-06-28T04:19:05.175733Z","shell.execute_reply.started":"2024-06-28T04:19:04.956732Z","shell.execute_reply":"2024-06-28T04:19:05.174700Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-06-28T04:19:08.654782Z","iopub.execute_input":"2024-06-28T04:19:08.655132Z","iopub.status.idle":"2024-06-28T04:19:08.871812Z","shell.execute_reply.started":"2024-06-28T04:19:08.655102Z","shell.execute_reply":"2024-06-28T04:19:08.870794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport math\nimport random\n\nfrom collections import defaultdict\n\nn_repetitions = 17 if PRIVATE else 4\nTOTAL_TOKENS = 2048 # if PRIVATE else 512","metadata":{"execution":{"iopub.status.busy":"2024-06-28T04:19:10.604726Z","iopub.execute_input":"2024-06-28T04:19:10.605133Z","iopub.status.idle":"2024-06-28T04:19:10.609687Z","shell.execute_reply.started":"2024-06-28T04:19:10.605105Z","shell.execute_reply":"2024-06-28T04:19:10.608708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if True:\n\n    MODEL_PATH = \"/kaggle/input/deepseek-math\" #\"/kaggle/input/gemma/transformers/7b-it/1\"\n    DEEP = True\n\n    config = AutoConfig.from_pretrained(MODEL_PATH)\n    config.gradient_checkpointing = True\n\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n\n    device_map = \"auto\"\n    # device_map = {ii:jj for (ii,jj) in device_map}\n\n    if QUANT:\n        from transformers import BitsAndBytesConfig\n        quantization_config = BitsAndBytesConfig(\n            load_in_4bit = True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch.bfloat16,\n            bnb_4bit_use_double_quant=True,\n        )\n        model = AutoModelForCausalLM.from_pretrained(\n            MODEL_PATH,\n            device_map=\"sequential\",\n            torch_dtype=\"auto\",\n            trust_remote_code=True, \n            quantization_config=quantization_config,\n            config=config\n        )\n    else:  \n        model = AutoModelForCausalLM.from_pretrained(\n            MODEL_PATH,\n            device_map=device_map,\n            torch_dtype=\"auto\",\n            trust_remote_code=True,\n            #quantization_config=quantization_config,\n            config=config\n        )\n    \n    pipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype='auto',\n    device_map=device_map,\n)\n    from transformers import StoppingCriteriaList\n\n    class StoppingCriteriaSub(StoppingCriteria):\n        def __init__(self, stops = [], encounters=1):\n            super().__init__()\n            self.stops = [stop.to(\"cuda\") for stop in stops]\n\n        def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n            for stop in self.stops:\n                last_token = input_ids[0][-len(stop):]\n                if torch.all(torch.eq(stop,last_token)):\n                    return True\n            return False\n\n\n    stop_words = [\"```output\", \"```python\", \"```\\nOutput\" , \")\\n```\" , \"``````output\"] #,  \n    stop_words_ids = [tokenizer(stop_word, return_tensors='pt', add_special_tokens=False)['input_ids'].squeeze() for stop_word in stop_words]\n    stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words_ids)])\n    \n    model.dtype, model.hf_device_map","metadata":{"execution":{"iopub.status.busy":"2024-06-28T04:19:13.063271Z","iopub.execute_input":"2024-06-28T04:19:13.063617Z","iopub.status.idle":"2024-06-28T04:21:53.145344Z","shell.execute_reply.started":"2024-06-28T04:19:13.063587Z","shell.execute_reply":"2024-06-28T04:21:53.144595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"code = \"\"\"Below is a math problem you are to solve (positive numerical answer):\n\\\"{}\\\"\nTo accomplish this, first determine a sympy-based approach for solving the problem by listing each step to take and what functions need to be called in each step. Be clear so even an idiot can follow your instructions, and remember, your final answer should be positive integer, not an algebraic expression!\nWrite the entire script covering all the steps (use comments and document it well) and print the result. After solving the problem, output the final numerical answer within \\\\boxed{}.\n\nApproach:\"\"\"\n\n\ncot = \"\"\"Below is a math problem you are to solve (positive numerical answer!):\n\\\"{}\\\"\nAnalyze this problem and think step by step to come to a solution with programs. After solving the problem, output the final numerical answer within \\\\boxed{}.\\n\\n\"\"\"\n\npromplt_options = [code,cot]","metadata":{"execution":{"iopub.status.busy":"2024-06-28T04:22:12.619989Z","iopub.execute_input":"2024-06-28T04:22:12.620908Z","iopub.status.idle":"2024-06-28T04:22:12.627143Z","shell.execute_reply.started":"2024-06-28T04:22:12.620865Z","shell.execute_reply":"2024-06-28T04:22:12.625986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed):\n    import random, os\n    import numpy as np\n    import torch\n    \n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    set_seed(seed)\n    \nseed_everything(42)","metadata":{"execution":{"iopub.status.busy":"2024-06-28T04:22:15.213473Z","iopub.execute_input":"2024-06-28T04:22:15.214329Z","iopub.status.idle":"2024-06-28T04:22:15.220243Z","shell.execute_reply.started":"2024-06-28T04:22:15.214296Z","shell.execute_reply":"2024-06-28T04:22:15.219273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nfrom collections import defaultdict\nfrom collections import Counter\n\nfrom numpy.random import choice\nimport numpy as np\n\ntool_instruction = '\\n\\nPlease integrate natural language reasoning with programs to solve the above problem, and put your final numerical answer within \\\\boxed{}.\\nNote that the intermediary calculations may be real numbers, but the final numercal answer would always be an integer.'\n\n\n#tool_instruction = \" The answer should be given as a non-negative modulo 1000.\"\n#tool_instruction += '\\nPlease integrate natural language reasoning with programs to solve the problem above, and put your final answer within \\\\boxed{}.'\n\ntemperature = 0.9\ntop_p = 1.0\n\ntemperature_coding = 0.9\ntop_p_coding = 1.0\n\n   \ntotal_results = {}\ntotal_answers = {}\nbest_stats = {}\ntotal_outputs = {}\nquestion_type_counts = {}\nstarting_counts = (2,3)","metadata":{"papermill":{"duration":34.259365,"end_time":"2024-02-29T09:37:05.548829","exception":false,"start_time":"2024-02-29T09:36:31.289464","status":"completed"},"tags":[],"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-06-28T04:22:18.086108Z","iopub.execute_input":"2024-06-28T04:22:18.086498Z","iopub.status.idle":"2024-06-28T04:22:18.093624Z","shell.execute_reply.started":"2024-06-28T04:22:18.086466Z","shell.execute_reply":"2024-06-28T04:22:18.092579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Making a function for prediction!","metadata":{}},{"cell_type":"code","source":"extra_time = 0\nallowed = 0\ndef predict(problem):\n    seed_everything(69)\n    \n    temperature = 0.9\n    top_p = 1.0\n\n    temperature_coding = 0.9\n    top_p_coding = 1.0\n\n   \n    total_results = {}\n    total_answers = {}\n    best_stats = {}\n    total_outputs = {}\n    question_type_counts = {}\n    starting_counts = (2,3)\n    i = 0\n    \n    global n_repetitions,TOTAL_TOKENS,model,tokenizer,USE_PAST_KEY,NOTEBOOK_START_TIME,promplt_options,code,cot,extra_time,allowed\n    \n    \n    if time.time()-NOTEBOOK_START_TIME>=32200:\n        return 0\n    \n    PROBLEM_START_TIME = time.time()\n    allowed = min(extra_time,600)\n    \n    for jj in tqdm(range(n_repetitions)):\n        best, best_count = best_stats.get(i,(-1,-1))\n        if best_count>np.sqrt(jj):\n            print(\"SKIPPING CAUSE ALREADY FOUND BEST\")\n            continue\n        \n        if time.time()-PROBLEM_START_TIME>=780+allowed:\n            \n            extra = (time.time()-PROBLEM_START_TIME)-780\n            extra_time-=max(0,extra)\n            \n            return best_stats[0][0]\n        \n        if time.time()-NOTEBOOK_START_TIME>=32200:\n            return best_stats[0][0]\n\n        outputs = total_outputs.get(i,[])\n        text_answers, code_answers = question_type_counts.get(i,starting_counts)\n        results = total_results.get(i,[])\n        answers = total_answers.get(i,[])  \n        \n        for _ in range(5):\n            torch.cuda.empty_cache()\n            gc.collect()\n            time.sleep(0.2)\n        \n        try:\n            ALREADY_GEN = 0\n            code_error = None\n            code_error_count = 0\n            code_output = -1\n            #initail_message = problem  + tool_instruction \n            counts = np.array([text_answers,code_answers])\n\n            draw = choice(promplt_options, 1,\n                          p=counts/counts.sum())\n\n            initail_message = draw[0].format(problem,\"{}\")            \n            prompt = f\"User: {initail_message}\"\n\n            current_printed = len(prompt)\n            print(f\"{jj}_{prompt}\\n\")\n\n            model_inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n            input_len = len(model_inputs['input_ids'][0])\n\n            generation_output = model.generate(**model_inputs, \n                                               max_new_tokens=TOTAL_TOKENS-ALREADY_GEN,\n                                               return_dict_in_generate=USE_PAST_KEY,\n                                               do_sample = True,\n                                               temperature = temperature,\n                                               top_p = top_p,\n                                               num_return_sequences=1, stopping_criteria = stopping_criteria)\n\n            if USE_PAST_KEY:\n                output_ids = generation_output.sequences[0]\n            else:\n                output_ids = generation_output[0]\n            decoded_output = tokenizer.decode(output_ids, skip_special_tokens=True)\n            print(f\"{decoded_output[current_printed:]}\\n\")\n            current_printed += len(decoded_output[current_printed:])\n            cummulative_code = \"\"\n            \n            stop_word_cond = False\n            for stop_word in stop_words:\n                stop_word_cond = stop_word_cond or (decoded_output[-len(stop_word):]==stop_word)\n                \n                \n            while (stop_word_cond) and (ALREADY_GEN<(TOTAL_TOKENS)):\n\n                if (decoded_output[-len(\"```python\"):]==\"```python\"):\n                    temperature_inner=temperature_coding\n                    top_p_inner = top_p_coding\n                    prompt = decoded_output\n                else:\n                    temperature_inner=temperature\n                    top_p_inner = top_p\n                    try:\n                        if (decoded_output[-len(\"``````output\"):]==\"``````output\"):\n                            code_text = decoded_output.split('```python')[-1].split(\"``````\")[0]\n                        else:\n                            code_text = decoded_output.split('```python')[-1].split(\"```\")[0]\n                        \n\n                        cummulative_code+=code_text\n                        code_output, CODE_STATUS = process_code(cummulative_code, return_shell_output=True)\n                        print('CODE RESULTS', code_output)\n\n                        if code_error==code_output:\n                            code_error_count+=1\n                        else:\n                            code_error=code_output\n                            code_error_count = 0\n\n                        if not CODE_STATUS:\n                            cummulative_code = cummulative_code[:-len(code_text)]\n\n                            if code_error_count>=1:\n                                print(\"REPEATED ERRORS\")\n                                break\n\n                    except Exception as e:\n                        print(e)\n                        print('ERROR PARSING CODE')\n                        code_output = -1\n\n                    if code_output!=-1:\n                        if (decoded_output[-len(\")\\n```\"):]==\")\\n```\"):\n                            prompt = decoded_output+'```output\\n'+str(code_output)+'\\n```\\n'\n                        else:\n                            prompt = decoded_output+'\\n'+str(code_output)+'\\n```\\n'\n                    else:\n                        prompt = decoded_output\n                        cummulative_code=\"\"\n                model_inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n                ALREADY_GEN =  len(model_inputs['input_ids'][0])-input_len\n\n                if USE_PAST_KEY:\n                    old_values = generation_output.past_key_values\n                else:\n                    old_values = None\n\n                generation_output = model.generate(**model_inputs, \n                                                   max_new_tokens=TOTAL_TOKENS-ALREADY_GEN, \n                                                   return_dict_in_generate=USE_PAST_KEY,\n                                                   past_key_values=old_values,\n                                                   do_sample = True,\n                                                   temperature = temperature_inner,\n                                                   top_p = top_p_inner,\n                                                   num_return_sequences=1, stopping_criteria = stopping_criteria)\n                if USE_PAST_KEY:\n                    output_ids = generation_output.sequences[0]\n                else:\n                    output_ids = generation_output[0]\n                decoded_output = tokenizer.decode(output_ids, skip_special_tokens=True)\n                print(f\"\\nINTERMEDIATE OUT :\\n{decoded_output[current_printed:]}\\n\")\n                current_printed+=len(decoded_output[current_printed:])\n                \n                stop_word_cond = False\n                for stop_word in stop_words:\n                    stop_word_cond = stop_word_cond or (decoded_output[-len(stop_word):]==stop_word)\n            if USE_PAST_KEY:\n                output_ids = generation_output.sequences[0]\n            else:\n                output_ids = generation_output[0]\n\n            raw_output = tokenizer.decode(output_ids[input_len:], skip_special_tokens=True)\n            #print(f\"\\n\\nOutput :\\n{raw_output}\\n\")                            \n            result_output = process_text_output(raw_output)\n            \n            try:\n                code_output = round(float(eval(code_output))) % 1000\n            except Exception as e:\n                print(e,'final_eval')\n                code_output = -1\n        except Exception as e:\n            print(e,\"5\")\n            result_output, code_output = -1, -1\n\n        if code_output!=-1:\n            outputs.append(code_output)\n            code_answers+=1\n\n        if result_output!=-1:\n            outputs.append(result_output)\n            text_answers+=1\n\n        if len(outputs) > 0:\n            occurances = Counter(outputs).most_common()\n            print(occurances)\n            if occurances[0][1] > best_count:\n                print(\"GOOD ANSWER UPDATED!\")\n                best = occurances[0][0]\n                best_count = occurances[0][1]\n            if occurances[0][1] > 5:\n                print(\"ANSWER FOUND!\")\n                break\n\n        results.append(result_output)\n        answers.append(code_output)\n        \n        best_stats[i] = (best, best_count) \n        question_type_counts[i] = (text_answers, code_answers)\n        total_outputs[i] = outputs\n        \n        total_results[i] = results\n        total_answers[i] = answers\n\n        print(\"code_answers\",code_answers-starting_counts[1],\"text_answers\",text_answers-starting_counts[0])\n    remaining = 780-(time.time()-PROBLEM_START_TIME)\n    #print(\"remaining\", remaining)\n    remaining = max(0,remaining)\n    extra_time+=remaining\n    #print(\"extra_time\", extra_time)\n    return best_stats[0][0]","metadata":{"execution":{"iopub.status.busy":"2024-06-28T04:22:22.820831Z","iopub.execute_input":"2024-06-28T04:22:22.821189Z","iopub.status.idle":"2024-06-28T04:22:22.863705Z","shell.execute_reply.started":"2024-06-28T04:22:22.821159Z","shell.execute_reply":"2024-06-28T04:22:22.862721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Prediction on the train set to see if the function works!**","metadata":{}},{"cell_type":"code","source":"if not PRIVATE:\n    df = pd.read_csv('/kaggle/input/ai-mathematical-olympiad-prize/train.csv')\n    \n    df['model_answer'] = df['problem'].apply(lambda x:predict(x))\n    df['match'] = df.answer == df.model_answer\n    print(f'{df.match.sum()} matches in {len(df)} examples')\nelse:\n    df = pd.read_csv('/kaggle/input/ai-mathematical-olympiad-prize/train.csv')\n    \n    df['model_answer'] = df['problem'].apply(lambda x:predict(x))\n    df['match'] = df.answer == df.model_answer\n    print(f'{df.match.sum()} matches in {len(df)} examples')","metadata":{"execution":{"iopub.status.busy":"2024-06-28T04:22:31.079831Z","iopub.execute_input":"2024-06-28T04:22:31.080687Z","iopub.status.idle":"2024-06-28T04:54:07.008578Z","shell.execute_reply.started":"2024-06-28T04:22:31.080624Z","shell.execute_reply":"2024-06-28T04:54:07.007118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission with the new API","metadata":{}},{"cell_type":"code","source":"import aimo\n\nenv = aimo.make_env()\niter_test = env.iter_test()","metadata":{"execution":{"iopub.status.busy":"2024-06-27T09:31:14.673225Z","iopub.execute_input":"2024-06-27T09:31:14.673582Z","iopub.status.idle":"2024-06-27T09:31:14.696449Z","shell.execute_reply.started":"2024-06-27T09:31:14.673550Z","shell.execute_reply":"2024-06-27T09:31:14.695552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if PRIVATE:\n    for test, sample_submission in iter_test:\n        sample_submission['answer'] = predict(test['problem'].values[0])\n        env.predict(sample_submission)\n        print(test)\n        print(sample_submission)\n        \nelse:\n    for test, sample_submission in iter_test:\n        sample_submission['answer'] = predict(test['problem'].values[0])\n        env.predict(sample_submission)\n        print(test)\n        print(sample_submission)","metadata":{"execution":{"iopub.status.busy":"2024-06-27T09:31:17.397030Z","iopub.execute_input":"2024-06-27T09:31:17.397376Z","iopub.status.idle":"2024-06-27T09:35:51.869461Z","shell.execute_reply.started":"2024-06-27T09:31:17.397347Z","shell.execute_reply":"2024-06-27T09:35:51.868391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('code.py', 'w') as fout:\n    fout.write(\"print('done')\")\n\nbatcmd = 'timeout 7 ' + sys.executable + ' code.py'\ntry:\n    shell_output = subprocess.check_output(batcmd, shell=True).decode('utf8')\n    print(shell_output)\nexcept:\n    pass","metadata":{"execution":{"iopub.status.busy":"2024-06-27T09:36:16.775340Z","iopub.execute_input":"2024-06-27T09:36:16.776058Z","iopub.status.idle":"2024-06-27T09:36:16.890277Z","shell.execute_reply.started":"2024-06-27T09:36:16.776024Z","shell.execute_reply":"2024-06-27T09:36:16.889330Z"},"trusted":true},"execution_count":null,"outputs":[]}]}